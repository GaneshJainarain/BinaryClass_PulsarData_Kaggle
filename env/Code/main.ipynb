{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground Series - Season 3, Episode 10\n",
    "### `Binary Classification with a Pulsar Dataset`\n",
    "\n",
    "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Pulsar Classification. \n",
    "Objective is to predict the probability of Class\n",
    "(whether the observation is a pulsar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Files`\n",
    "\n",
    "- train.csv - the training dataset; Class is the (binary) target\n",
    "- test.csv - the test dataset;\n",
    "- sample_submission.csv - a sample submission file in the correct format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Data Science Framework & Process`\n",
    "\n",
    "- Define the Problem\n",
    "- Gather the Data\n",
    "- Prepare Data for Consumption\n",
    "- Perform Exploratory Analysis\n",
    "- Model Data\n",
    "- Validate and Implement Data Model\n",
    "- Optimize and Strategize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Defining Our Problem`\n",
    "\n",
    "`Predict the probability of the variable Class`\n",
    "\n",
    "Pulsars are rapidly spinning neutron stars, extremely dense stars composed almost entirely of neutrons and having a diameter of only 20 km (12 miles) or less. Pulsar masses range between 1.18 and 1.97 times that of the Sun, but most pulsars have a mass 1.35 times that of the Sun.\n",
    "\n",
    "Pulsars are a rare type of Neutron star that produce radio emission detectable here on Earth. They are of considerable scientific interest as probes of space-time, the inter-stellar medium, and states of matter . Neutron stars are very dense, and have short, regular rotational periods. This produces a very precise interval between pulses that ranges from milliseconds to seconds for an individual pulsar. Pulsars are believed to be one of the candidates for the source of ultra-high-energy cosmic rays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Gather the Data`\n",
    "\n",
    "The data is given to us via Kaggle, Download at:\n",
    "(Kaggle Pulsar Dataset)\n",
    "https://www.kaggle.com/competitions/playground-series-s3e10/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Prepare Data for Consumption`\n",
    "\n",
    "#### `Import Libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.3 (v3.8.3:6f8c8320e9, May 13 2020, 16:29:34) \n",
      "[Clang 6.0 (clang-600.0.57)]\n",
      "pandas version: 1.5.3\n",
      "matplotlib version: 3.7.1\n",
      "NumPy version: 1.24.2\n",
      "SciPy version: 1.10.1\n",
      "IPython version: 8.11.0\n",
      "scikit-learn version: 1.2.1\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "#misc libraries\n",
    "import random\n",
    "import time\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Load Data Modelling Libraries`\n",
    "We will use the popular scikit-learn library to develop our `machine learning algorithms`. In sklearn, algorithms are called Estimators and implemented in their own classes. For `data visualization`, we will use the matplotlib and seaborn library. Below are common classes to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading of Data Modelling Libraries Complete ✅\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8\n",
    "\n",
    "print(\"Loading of Data Modelling Libraries Complete ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Our Data`\n",
    "\n",
    "\n",
    "`17898 entries`\n",
    "\n",
    "Data can be useful for prediction models of classification.\n",
    "\n",
    "`COLUMNS:`\n",
    "Based on Integrated Profile of Observation\n",
    "\n",
    "- `Mean_Integrated`: Mean of Observations\n",
    "\n",
    "- `SD`: Standard deviation of Observations\n",
    "\n",
    "- `EK`: Excess kurtosis of Observations\n",
    "\n",
    "- `Skewness`: In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. Skewness of Observations.\n",
    "\n",
    "- `Mean _ DMSNR _ Curve`: Mean of DM SNR CURVE of Observations\n",
    "\n",
    "- `SD _ DMSNR _ Curve`: Standard deviation of DM SNR CURVE of Observations\n",
    "\n",
    "- `EK _ DMSNR _ Curve`: Excess kurtosis of DM SNR CURVE of Observations\n",
    "\n",
    "- `Skewness _ DMSNR _ Curve`: Skewness of DM SNR CURVE of Observations\n",
    "\n",
    "- `Class`: Class 0 - 1\n",
    "\n",
    "`WHAT IS DM SNR CURVE:`\n",
    "\n",
    "Radio waves emitted from pulsars reach earth after traveling long distances in space which is filled with free electrons. \n",
    "The important point is that pulsars emit a wide range of frequencies, and the amount by which the electrons slow down the wave depends on the frequency. \n",
    "Waves with higher frequency are sowed down less as compared to waves with higher frequency. It means dispersion.\n",
    "\n",
    "`TARGET`:\n",
    "\n",
    "`Class`\n",
    "   - 0 -- It is not\n",
    "   - 1 -- It is\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Train/Test`\n",
    "\n",
    "A dataset should be broken into 3 splits: \n",
    "\n",
    "    - train\n",
    "    - test \n",
    "    - (final) validation\n",
    "\n",
    "the test file provided is the validation file for competition submission\n",
    "we will split the train set into train and test data in future sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('/Users/richeyjay/Desktop/BinaryClass_PulsarData_Kaggle/env/Data/train.csv')\n",
    "data_val  = pd.read_csv('/Users/richeyjay/Desktop/BinaryClass_PulsarData_Kaggle/env/Data/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `To play with our data we'll create a copy`\n",
    "Remember python assignment or equal passes by reference vs values, so we use the copy function:\n",
    "     https://stackoverflow.com/questions/46327494/python-pandas-dataframe-copydeep-false-vs-copydeep-true-vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data_raw.copy(deep = True)\n",
    "\n",
    "#however passing by reference is convenient, because we can clean both datasets at once\n",
    "data_cleaner = [data1, data_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117564 entries, 0 to 117563\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   id                    117564 non-null  int64  \n",
      " 1   Mean_Integrated       117564 non-null  float64\n",
      " 2   SD                    117564 non-null  float64\n",
      " 3   EK                    117564 non-null  float64\n",
      " 4   Skewness              117564 non-null  float64\n",
      " 5   Mean_DMSNR_Curve      117564 non-null  float64\n",
      " 6   SD_DMSNR_Curve        117564 non-null  float64\n",
      " 7   EK_DMSNR_Curve        117564 non-null  float64\n",
      " 8   Skewness_DMSNR_Curve  117564 non-null  float64\n",
      " 9   Class                 117564 non-null  int64  \n",
      "dtypes: float64(8), int64(2)\n",
      "memory usage: 9.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#preview data\n",
    "print(data_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  Mean_Integrated         SD        EK  Skewness  \\\n",
      "104860  104860       136.039062  49.262779 -0.067151  0.138945   \n",
      "70339    70339       123.617188  50.337600  0.157928 -0.181856   \n",
      "81349    81349        98.093750  41.191170  0.493438  0.965385   \n",
      "91075    91075        84.773438  51.157901  1.290134  1.829378   \n",
      "111903  111903       108.742188  52.081919  0.524977  0.089749   \n",
      "\n",
      "        Mean_DMSNR_Curve  SD_DMSNR_Curve  EK_DMSNR_Curve  \\\n",
      "104860          7.989967       34.656981        4.683489   \n",
      "70339         120.844482       68.078558       -0.866049   \n",
      "81349           2.306856       18.061868        9.058349   \n",
      "91075          31.661371       65.283436        1.653938   \n",
      "111903          2.884615       18.374312        7.913979   \n",
      "\n",
      "        Skewness_DMSNR_Curve  Class  \n",
      "104860             21.835699      0  \n",
      "70339              -0.681819      0  \n",
      "81349              90.536536      0  \n",
      "91075               1.851652      1  \n",
      "111903             72.571592      0  \n"
     ]
    }
   ],
   "source": [
    "#Getting a small sample of our data values, \n",
    "#columns and rows\n",
    "print(data_raw.sample(5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting`\n",
    "\n",
    "In this stage, we will clean our data by `1)` correcting aberrant values and outliers, `2)` completing missing information, `3)` creating new features for analysis, and `4)` converting fields to the correct format for calculations and presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns with null values:\n",
      " id                      0\n",
      "Mean_Integrated         0\n",
      "SD                      0\n",
      "EK                      0\n",
      "Skewness                0\n",
      "Mean_DMSNR_Curve        0\n",
      "SD_DMSNR_Curve          0\n",
      "EK_DMSNR_Curve          0\n",
      "Skewness_DMSNR_Curve    0\n",
      "Class                   0\n",
      "dtype: int64\n",
      "----------\n",
      "Test/Validation columns with null values:\n",
      " id                      0\n",
      "Mean_Integrated         0\n",
      "SD                      0\n",
      "EK                      0\n",
      "Skewness                0\n",
      "Mean_DMSNR_Curve        0\n",
      "SD_DMSNR_Curve          0\n",
      "EK_DMSNR_Curve          0\n",
      "Skewness_DMSNR_Curve    0\n",
      "dtype: int64\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Mean_Integrated</th>\n",
       "      <th>SD</th>\n",
       "      <th>EK</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Mean_DMSNR_Curve</th>\n",
       "      <th>SD_DMSNR_Curve</th>\n",
       "      <th>EK_DMSNR_Curve</th>\n",
       "      <th>Skewness_DMSNR_Curve</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>117564.000000</td>\n",
       "      <td>117564.000000</td>\n",
       "      <td>117564.000000</td>\n",
       "      <td>117564.000000</td>\n",
       "      <td>117564.000000</td>\n",
       "      <td>117564.000000</td>\n",
       "      <td>117564.000000</td>\n",
       "      <td>117564.000000</td>\n",
       "      <td>117564.000000</td>\n",
       "      <td>117564.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>58781.500000</td>\n",
       "      <td>111.248300</td>\n",
       "      <td>46.713535</td>\n",
       "      <td>0.503498</td>\n",
       "      <td>1.886385</td>\n",
       "      <td>11.962921</td>\n",
       "      <td>26.190678</td>\n",
       "      <td>8.037488</td>\n",
       "      <td>93.881076</td>\n",
       "      <td>0.093285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>33937.947861</td>\n",
       "      <td>24.906474</td>\n",
       "      <td>6.102941</td>\n",
       "      <td>1.127093</td>\n",
       "      <td>6.515466</td>\n",
       "      <td>26.719946</td>\n",
       "      <td>20.041937</td>\n",
       "      <td>3.840980</td>\n",
       "      <td>79.962110</td>\n",
       "      <td>0.290833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.054688</td>\n",
       "      <td>24.783273</td>\n",
       "      <td>-1.730782</td>\n",
       "      <td>-1.791886</td>\n",
       "      <td>0.213211</td>\n",
       "      <td>7.370432</td>\n",
       "      <td>-2.597872</td>\n",
       "      <td>-1.976976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29390.750000</td>\n",
       "      <td>104.546875</td>\n",
       "      <td>43.443390</td>\n",
       "      <td>0.049761</td>\n",
       "      <td>-0.188956</td>\n",
       "      <td>2.090301</td>\n",
       "      <td>14.955405</td>\n",
       "      <td>6.742911</td>\n",
       "      <td>49.409136</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>58781.500000</td>\n",
       "      <td>116.664062</td>\n",
       "      <td>47.478932</td>\n",
       "      <td>0.186498</td>\n",
       "      <td>0.091720</td>\n",
       "      <td>2.808528</td>\n",
       "      <td>18.164924</td>\n",
       "      <td>8.442883</td>\n",
       "      <td>83.421375</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>88172.250000</td>\n",
       "      <td>126.296875</td>\n",
       "      <td>50.862718</td>\n",
       "      <td>0.395620</td>\n",
       "      <td>0.691613</td>\n",
       "      <td>4.122910</td>\n",
       "      <td>24.732218</td>\n",
       "      <td>10.003237</td>\n",
       "      <td>122.093290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>117563.000000</td>\n",
       "      <td>189.367188</td>\n",
       "      <td>93.602933</td>\n",
       "      <td>7.879628</td>\n",
       "      <td>65.385974</td>\n",
       "      <td>217.371238</td>\n",
       "      <td>109.890785</td>\n",
       "      <td>34.539844</td>\n",
       "      <td>1191.000837</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  Mean_Integrated             SD             EK  \\\n",
       "count  117564.000000    117564.000000  117564.000000  117564.000000   \n",
       "mean    58781.500000       111.248300      46.713535       0.503498   \n",
       "std     33937.947861        24.906474       6.102941       1.127093   \n",
       "min         0.000000         6.054688      24.783273      -1.730782   \n",
       "25%     29390.750000       104.546875      43.443390       0.049761   \n",
       "50%     58781.500000       116.664062      47.478932       0.186498   \n",
       "75%     88172.250000       126.296875      50.862718       0.395620   \n",
       "max    117563.000000       189.367188      93.602933       7.879628   \n",
       "\n",
       "            Skewness  Mean_DMSNR_Curve  SD_DMSNR_Curve  EK_DMSNR_Curve  \\\n",
       "count  117564.000000     117564.000000   117564.000000   117564.000000   \n",
       "mean        1.886385         11.962921       26.190678        8.037488   \n",
       "std         6.515466         26.719946       20.041937        3.840980   \n",
       "min        -1.791886          0.213211        7.370432       -2.597872   \n",
       "25%        -0.188956          2.090301       14.955405        6.742911   \n",
       "50%         0.091720          2.808528       18.164924        8.442883   \n",
       "75%         0.691613          4.122910       24.732218       10.003237   \n",
       "max        65.385974        217.371238      109.890785       34.539844   \n",
       "\n",
       "       Skewness_DMSNR_Curve          Class  \n",
       "count         117564.000000  117564.000000  \n",
       "mean              93.881076       0.093285  \n",
       "std               79.962110       0.290833  \n",
       "min               -1.976976       0.000000  \n",
       "25%               49.409136       0.000000  \n",
       "50%               83.421375       0.000000  \n",
       "75%              122.093290       0.000000  \n",
       "max             1191.000837       1.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = pd.read_csv('/Users/richeyjay/Desktop/BinaryClass_PulsarData_Kaggle/env/Data/train.csv')\n",
    "data_val  = pd.read_csv('/Users/richeyjay/Desktop/BinaryClass_PulsarData_Kaggle/env/Data/test.csv')\n",
    "\n",
    "data1 = data_raw.copy(deep = True)\n",
    "data_cleaner = [data1, data_val]\n",
    "\n",
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n',data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Correcting`\n",
    "\n",
    "Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Completing`\n",
    "\n",
    "As we can see there are no null 'NaN' values in our dataset.\n",
    "\n",
    "Missing values can be bad, because some algorithms don't know how-to handle `null values` and will fail. While others, like decision trees, can handle null values.\n",
    "\n",
    "Thus, it's important to fix before we start modeling, because we will compare and contrast several models. There are two common methods, either `delete the record` or `populate the missing value` using a reasonable input. \n",
    "\n",
    "It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to `impute missing values`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Creating` \n",
    "\n",
    "Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Converting` \n",
    "\n",
    "Our categorical data imported as floats and ints, which is good because machine learning models do well with numerical inputs. For this dataset, we will not convert any data-types seeing they aren't of object types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Categorical Values`\n",
    "\n",
    "In many practical Data Science activities, the data set will contain `categorical variables`. These variables are typically stored as text values which represent various traits. \n",
    "\n",
    "Some examples include color (“Red”, “Yellow”, “Blue”), size (“Small”, “Medium”, “Large”) or geographic designations (State or Country). Regardless of what the value is used for, the challenge is determining how to use this data in the analysis. \n",
    "\n",
    "Many machine learning algorithms can support categorical values without further manipulation but there are many more algorithms that do not. Therefore, the analyst is faced with the challenge of figuring out how to turn these `text attributes into numerical values` for further processing.\n",
    "\n",
    "As we can see below this particular dataset has only `numerical values` therefore we do not need to do any form of encoding, if the data was a bunch of Object types then we would have to use something like `LabelEncoder` or `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117564 entries, 0 to 117563\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   id                    117564 non-null  int64  \n",
      " 1   Mean_Integrated       117564 non-null  float64\n",
      " 2   SD                    117564 non-null  float64\n",
      " 3   EK                    117564 non-null  float64\n",
      " 4   Skewness              117564 non-null  float64\n",
      " 5   Mean_DMSNR_Curve      117564 non-null  float64\n",
      " 6   SD_DMSNR_Curve        117564 non-null  float64\n",
      " 7   EK_DMSNR_Curve        117564 non-null  float64\n",
      " 8   Skewness_DMSNR_Curve  117564 non-null  float64\n",
      " 9   Class                 117564 non-null  int64  \n",
      "dtypes: float64(8), int64(2)\n",
      "memory usage: 9.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#numerical values\n",
    "print(data_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Convert Formats`\n",
    "\n",
    "In this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X Y:  ['Class', 'Mean_Integrated', 'SD', 'EK', 'Skewness', 'Mean_DMSNR_Curve', 'SD_DMSNR_Curve', 'EK_DMSNR_Curve', 'Skewness_DMSNR_Curve'] \n",
      "\n",
      "--------------------------------------------------\n",
      "Bin X Y:  ['Class', 'Mean_Integrated', 'SD', 'EK', 'Skewness', 'Mean_DMSNR_Curve', 'SD_DMSNR_Curve', 'EK_DMSNR_Curve', 'Skewness_DMSNR_Curve'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define y variable aka target/outcome\n",
    "Target = ['Class']\n",
    "\n",
    "#define x variables for original features aka feature selection\n",
    "data1_x = ['Mean_Integrated','SD', 'EK', 'Skewness','Mean_DMSNR_Curve', 'SD_DMSNR_Curve', 'EK_DMSNR_Curve', 'Skewness_DMSNR_Curve'] \n",
    "#pretty name/values for charts\n",
    "\n",
    "#coded for algorithm calculation\n",
    "data1_x_calc = ['Mean_Integrated','SD', 'EK', 'Skewness','Mean_DMSNR_Curve', 'SD_DMSNR_Curve', 'EK_DMSNR_Curve', 'Skewness_DMSNR_Curve']\n",
    "data1_xy =  Target + data1_x\n",
    "print('Original X Y: ', data1_xy, '\\n')\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "#define x variables for original w/bin features to remove continuous variables\n",
    "data1_x_bin = ['Mean_Integrated','SD', 'EK', 'Skewness','Mean_DMSNR_Curve', 'SD_DMSNR_Curve', 'EK_DMSNR_Curve', 'Skewness_DMSNR_Curve'] \n",
    "data1_xy_bin = Target + data1_x_bin\n",
    "print('Bin X Y: ', data1_xy_bin, '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Split Training and Testing Data`\n",
    "\n",
    "The test file provided is really validation data for competition submission. So, we will use sklearn function to split the training data in two datasets; 75/25 split. This is important, so we don't overfit our model. Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the same dataset. \n",
    "\n",
    "It's important our algorithm has not seen the subset we will use to test, so it doesn't \"cheat\" by memorizing the answers. We will use sklearn's `train_test_split function`. In later sections we will also use sklearn's `cross validation` functions, that splits our dataset into train and test for data modeling comparison.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8f28ed50434129ac93f93d7e8fd1c29db31c6d5452ac362f3c95aa434902534"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
